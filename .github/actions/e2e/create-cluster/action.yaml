name: CreateCluster
description: 'Installs Go Downloads and installs Karpenter Dependencies'
inputs:
  account_id:
    description: "Account ID to access AWS"
    required: true
  role:
    description: "Role to access AWS"
    required: true
  region:
    description: "Region to access AWS"
    required: true
  cluster_name:
    description: 'Name of the cluster to be launched by eksctl'
    required: true
  k8s_version:
    description: 'Version of Kubernetes to use for the launched cluster'
    required: false
    default: "1.28"
  eksctl_version:
    description: "Version of eksctl to install"
    default: v0.160.0-rc.0
  ip_family:
    description: "IP Family of the cluster. Valid values are IPv4 or IPv6"
    required: false
    default: "IPv4"
  git_ref:
    description: "The git commit, tag, or branch to check out. Requires a corresponding Karpenter snapshot release"
    required: false
runs:
  using: "composite"
  steps:
  - name: configure aws credentials
    uses: aws-actions/configure-aws-credentials@v4.0.1
    with:
      role-to-assume: arn:aws:iam::${{ inputs.account_id }}:role/${{ inputs.role }}
      aws-region: ${{ inputs.region }}
      role-duration-seconds: 21600
  - uses: actions/checkout@v4
    with:
      ref: ${{ inputs.git_ref }}
  - uses: ./.github/actions/e2e/install-eksctl
    with:
      version: ${{ inputs.eksctl_version }}
  - name: create iam policies
    shell: bash
    run: |
      # Resolve the cloudformation path with fallback
      CLOUDFORMATION_PATH=website/content/en/preview/getting-started/getting-started-with-karpenter/cloudformation.yaml
      if [ ! -f $CLOUDFORMATION_PATH ]; then
        CLOUDFORMATION_PATH=website/content/en/preview/getting-started/getting-started-with-eksctl/cloudformation.yaml
      fi

      # Update the Cloudformation policy to add the permissionBoundary to the NodeRole
      yq -i '.Resources.KarpenterNodeRole.Properties.PermissionsBoundary="arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"' $CLOUDFORMATION_PATH

      aws iam create-service-linked-role --aws-service-name spot.amazonaws.com || true
      aws cloudformation deploy \
        --stack-name iam-${{ inputs.cluster_name }} \
        --template-file $CLOUDFORMATION_PATH \
        --capabilities CAPABILITY_NAMED_IAM \
        --parameter-overrides "ClusterName=${{ inputs.cluster_name }}" \
        --tags "testing/type=e2e" "testing/cluster=${{ inputs.cluster_name }}" "github.com/run-url=https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}" "karpenter.sh/discovery=${{ inputs.cluster_name }}"
  - name: deploy alpha instance profile
    shell: bash
    run: |
      aws iam create-instance-profile --instance-profile-name "KarpenterNodeInstanceProfile-${{ inputs.cluster_name }}" --tags Key=testing/type,Value=e2e Key=testing/cluster,Value=${{ inputs.cluster_name }} || true
      aws iam add-role-to-instance-profile --instance-profile-name "KarpenterNodeInstanceProfile-${{ inputs.cluster_name }}" --role-name "KarpenterNodeRole-${{ inputs.cluster_name }}" || true
  - name: deploy alpha policy
    shell: bash
    run: |
      export AWS_PARTITION=aws
      export AWS_REGION=${{ inputs.region }}
      export AWS_ACCOUNT_ID=${{ inputs.account_id }}
      export CLUSTER_NAME=${{ inputs.cluster_name }}
      
      POLICY_DOCUMENT=$(envsubst < .github/actions/e2e/create-cluster/alpha-controller-policy.json)
      POLICY_NAME="KarpenterControllerPolicy-Alpha-${CLUSTER_NAME}"
      echo "Creating policy $POLICY_NAME..."
      aws iam create-policy --policy-name "$POLICY_NAME" --policy-document "$POLICY_DOCUMENT" || true
  - name: create or upgrade cluster
    shell: bash
    run: |
      # Create or Upgrade the cluster based on whether the cluster already exists
      cmd="create"
      eksctl get cluster --name ${{ inputs.cluster_name }} && cmd="upgrade"

      cat << EOF >> clusterconfig.yaml
      ---
      apiVersion: eksctl.io/v1alpha5
      kind: ClusterConfig
      metadata:
        name: ${{ inputs.cluster_name }}
        region: ${{ inputs.region }}
        version: "${{ inputs.k8s_version }}"
        tags:
          karpenter.sh/discovery: ${{ inputs.cluster_name }}
          github.com/run-url: "https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          testing/type: "e2e"
      kubernetesNetworkConfig:
        ipFamily: ${{ inputs.ip_family }}
      managedNodeGroups:
        - instanceType: c5.4xlarge
          amiFamily: AmazonLinux2
          name: ${{ inputs.cluster_name }}-system-pool
          desiredCapacity: 2
          disableIMDSv1: true
          minSize: 2
          maxSize: 2
          iam:
            instanceRolePermissionsBoundary: "arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"
          taints:
          - key: CriticalAddonsOnly
            value: "true"
            effect: NoSchedule
      iam:
        serviceRolePermissionsBoundary: "arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"
        serviceAccounts:
          - metadata:
              name: karpenter
              namespace: karpenter
            attachPolicyARNs:
              - "arn:aws:iam::${{ inputs.account_id }}:policy/KarpenterControllerPolicy-${{ inputs.cluster_name }}"
              - "arn:aws:iam::${{ inputs.account_id }}:policy/KarpenterControllerPolicy-Alpha-${{ inputs.cluster_name }}"
            permissionsBoundary: "arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"
            roleName: karpenter-irsa-${{ inputs.cluster_name }}
            roleOnly: true
          - metadata:
              name: prometheus-kube-prometheus-prometheus
              namespace: prometheus
            attachPolicyARNs:
              - "arn:aws:iam::${{ inputs.account_id }}:policy/PrometheusWorkspaceIngestionPolicy"
            permissionsBoundary: "arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"
            roleName: prometheus-irsa-${{ inputs.cluster_name }}
            roleOnly: true
        withOIDC: true
      addons:
      - name: vpc-cni
        permissionsBoundary: "arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"
      - name: coredns
        permissionsBoundary: "arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"
      - name: kube-proxy
        permissionsBoundary: "arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"
      - name: aws-ebs-csi-driver
        permissionsBoundary: "arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"
        wellKnownPolicies:
          ebsCSIController: true
      EOF
      
      eksctl ${cmd} cluster -f clusterconfig.yaml
      
      # We need to call these update iamserviceaccount commands again since the "eksctl upgrade cluster" action
      # doesn't handle updates to IAM serviceaccounts correctly when the roles assigned to them change
      eksctl update iamserviceaccount -f clusterconfig.yaml --approve
      

  - name: tag oidc provider of the cluster
    if: always()
    shell: bash
    run: |
      oidc_id=$(aws eks describe-cluster --name ${{ inputs.cluster_name }} --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 3,4,5)
      arn="arn:aws:iam::${{ inputs.account_id }}:oidc-provider/${oidc_id}"
      aws iam tag-open-id-connect-provider --open-id-connect-provider-arn $arn \
         --tags Key=testing/type,Value=e2e  Key=github.com/run-url,Value=https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
  - name: give KarpenterNodeRole permission to bootstrap
    shell: bash
    run: |
      eksctl create iamidentitymapping \
      --username system:node:{{EC2PrivateDNSName}} \
      --cluster "${{ inputs.cluster_name }}" \
      --arn "arn:aws:iam::${{ inputs.account_id }}:role/KarpenterNodeRole-${{ inputs.cluster_name }}" \
      --group system:bootstrappers \
      --group system:nodes
  - name: cloudformation describe stack events
    shell: bash
    if: failure()
    run: |
      stack_names=$(aws cloudformation describe-stacks --query 'Stacks[?Tags[?Key == `karpenter.sh/discovery` && Value == `${{ inputs.CLUSTER_NAME }}`]].{StackName: StackName}' --output text)
      for stack_name in $stack_names; do
        echo "Stack Events for $stack_name:"
        aws cloudformation describe-stack-events --stack-name $stack_name
      done
